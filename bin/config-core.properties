###############################################################################
# These are database configuration properties
###############################################################################
webservice.app.url=http://localhost:8080/

# These are database configuration properties
# These are database configuration properties

mtclient.dgraph.host=MZHWLT16-HP
mtclient.dgraph.port=18271

mtclient.virtuoso.graphurl=http://10.0.0.74:8890/
mtclient.virtuoso.jdbcurl=jdbc:virtuoso://localhost:1111
mtclient.virtuoso.user=dba
mtclient.virtuoso.password=dba

mtclient.database.host=MZHWLT16-HP
# For cluster in this property you need to give comma separated ip or hostname all cluster nodes
#mtclient.database.host=172.0.0.71,172.0.0.72,172.0.0.73
mtclient.database.port=9160
# Property will be used only at run of Initialize new client
# This should be configured only when working on Cluster
mtclient.database.paritionStrategy=org.apache.cassandra.locator.SimpleStrategy
mtclient.database.replicationFactor=1

mtclient.solr.host=MZHWLT16-HP
#mtclient.solr.host=10.0.0.71:2181,10.0.0.72:2181,10.0.0.73:2181
# For cluster in this property you need to give comma separated zookeeper node ips with its port
#mtclient.solr.host=10.0.0.71:9983,10.0.0.72:9983
mtclient.solr.port=8983
# Property will be used only at run of Initialize new client
# This should be configured only when working on Cluster
mtclient.solr.numShards=1
mtclient.solr.replicationFactor=1

#Pagesize and Batchsize
mtclient.solr.read.maxrowsbatchsize=50000

# nodejs server connection properties
mtclient.nodejs.host=127.0.0.1
mtclient.nodejs.port=1337

# Report trigger client specific user id
mtclient.reportTrigger.userId=report.scheduler@pg.com

# Apache Ignite Cache Server host port. This should point to where ignite server running
# RIght now it is used for application level Caching
mtclient.apacheIgnite.hostwithport=10.0.0.74:47500..47509

# NodeJS Server 'USER-CHAT' Connection Properties
mtclient.userchat.nodejs.host=MZHWLT16-HP
mtclient.userchat.nodejs.port=9000

mtclient.database.global=globalMT
# NOT IN USE CURRENTLLY WITH CQL
mtclient.database.connectionpool.initialsize=10
# NOT IN USE CURRENTLLY WITH CQL
mtclient.database.connectionpool.minIdle=
# NOT IN USE CURRENTLLY WITH CQL
mtclient.database.connectionpool.maxIdle=100
# NOT IN USE CURRENTLLY WITH CQL
mtclient.database.connectionpool.maxActive=1000
#this is basically when all connections limit reached how much pool will wait for getting new connection
# NOT IN USE CURRENTLLY WITH CQL
mtclient.database.connectionpool.maxWait=-1
#for how much time client will wait for getting connection after that will throw exception, setting as 1 minute
mtclient.database.connectionpool.socketTimeOut=60000

# This property will enable disable webservices in deployed war file
# if true then system will make webservices soap calls from action class otherwise will directly call Webserviceimpl methods 
# Production servers we should always disable webservice call by setting flag as false
# Android app production server this should always be true as app will not work without webservices
mtclient.webservices.enable=false

# THESE PROPERTIES ARE CACHING PROPERTIES WHICH SHOULD BE TWEAKED DEPENDING ON HARDWARE WE ARE USING
# For each select node by Id it will push that in in memory cache, for each update,delete node by id will update cache AND each select preferably server by cache otherwise DB
mtclient.caching.nodeIdNodeCache.enable=false
mtclient.caching.metricPoolTagCache.enable=false
mtclient.caching.RelationTypeNameCache.enable=false
mtclient.caching.nodeIdIsACache.enable=false
mtclient.caching.OrgTablesCache.enable=false
mtclient.caching.periodCountCache.enable=false
mtclient.caching.solrCache.enable=false
mtclient.caching.tableCreatedCache.enable=false
mtclient.caching.globalContextGraph.enable=false
mtclient.caching.nodeSetCache.enable=false
mtclient.caching.relationIdCache.enable=false
mtclient.caching.relationTypeCache.enable=false
mtclient.caching.nodeNameNodeCache.enable=true
mtclient.caching.nodeNameTypeNodeSetCache.enable=false
mtclient.caching.datasourcePathNamesCache.enable=true
mtclient.caching.datasourcePathIdsCache.enable=true
mtclient.caching.datasourceNodeTypeNodePairCache.enable=true
mtclient.caching.NRNRelationCache.enable=true
mtclient.caching.NRNRelationHashCodeCache.enable=true
mtclient.caching.hashCodeIdCache.enable=true
mtclient.caching.keyDatasourceCache.enable=true
mtclient.caching.datasourceNodeTypeNodePairHashCodeCache.enable = true
mtclient.caching.nodeDatasourceNodeTypeCache.enable=true
mtclient.caching.partialSearchNodeSetCache.enable=true
# if you want to disable anything specific for a client read or write then introduce these properties
#mtclient.caching.partialSearchNodeSetCache.disable.allnodes.moodysgraph.read=true
#mtclient.caching.partialSearchNodeSetCache.disable.allnodes.moodysgraph.write=true
#mtclient.caching.partialSearchNodeSetCache.disable.allrelationtypes.moodysgraph.read=true
#mtclient.caching.partialSearchNodeSetCache.disable.allrelationtypes.moodysgraph.write=true
#mtclient.caching.partialSearchNodeSetCache.disable.insights.moodysgraph.read=false
#mtclient.caching.partialSearchNodeSetCache.disable.insights.moodysgraph.write=false

mtclient.caching.NodeTypeNodesCache.enable=true
mtclient.caching.virtualIdCache.enable=true
mtclient.caching.traversalAPIDataCache.enable=true

# Nov 2018 cisco DB version processing with heap getting bigger 30 to 50 GB Garbage Collector become bottlneck. Process hang or killed or abnormalslow
# This Java8 option is make those usecases work at cost of speed. Enabled these options whenever required. Default will be disabled.
mtclient.useoffheapmemory.datasourcesetup.enable=true
mtclient.useoffheapmemory.populatenode.enable=false
mtclient.useoffheapmemory.ekq.enable=false

mtclient.createrdf.datasourcesetup.enable=true
mtclient.createrdf.initializeclient.enable=true

# Analysis caching property
mtclient.caching.analysisCSVTableCache.enable=true
mtclient.caching.analysisCommonCache.enable=true

# Analysis MCAP Processor Wait Time Property
mtclient.mcapprocessor.derivedMCAP.waitTime=30

# THESE ARE TWO PROPERTIES which comes in picture whenever we run N queries of a table on cassandra in batch...So say jobsBatchSize=100 will tell system divide
# total no of queries to blocks of 100 and each block of query is ONE JOB which will be executed in a threads....Now N such jobs submitted say noofthreads=10
# means Ten threads keep looking for jobs to write in cassadnra....So each thread will write 100 queries in cassandra as jobsBatchSize=100
mtclient.cassandrabatchexecution.jobsBatchSize=500
# exactly same as mtclient.cassandrabatchexecution.jobsBatchSize BUT it will apply for only one table "contextSequence" because insert queries to that table are huge
mtclient.cassandrabatchexecution.contextsequence.jobsBatchSize=500
mtclient.cassandrabatchexecution.noofthreads=1
#This property is used when executing queries using IN clause, this value represents the number of elements allowed in a IN clause.
#metricresult : Only used in metricresult
#graph : Only used in graph functions library
mtclient.cassandrabatchexecution.metricresult.inClauseBatchSize=100
mtclient.cassandrabatchexecution.graph.inClauseBatchSize=100


# TO BE DELETED TO BE DELETED TO BE DELETED TO BE DELETED
#CLESS
#if(idList.size() > 19 && cartesianElementList.size() != 60) {  // 26000
#if(idList.size() > 50) {     // 1 crore
#if(idList.size() > 30) {     // 2.5 lakh
#if(idList.size() > 10) {     // 441
mtclient.temptestingproperty1=
mtclient.temptestingproperty2=

###############################################################################
# These are properties required for APPLICATION
###############################################################################


###############################################################################
# PARALLEL LOGICS REQUIRED PROPERTIES
###############################################################################
# During testing i had observed that if we push more jobs like 6000-8000 then it kind of hangs SO TWEAK this as per hardware, can be higher hit&trial...will change after more use 
mtclient.parallel.common.acrossMachines.jppfMaxJobLimit=5000
# On click of Administration Concole / MezzureAdmin / Context Tag SimilarTo AND metricdefintion tagupload case there is logic which loads all system tags to check whether tag's uploaded in file is new or not
# Loading all system tags can be problematic so this flag will help us..If no of tags to be uploaded are more than flag value then only it will load all system tags
# If our input tags are in thousand then we do not need logic of loading all system tags as overwrite of thousand tags will work faster
# If our inpurt tags are in lakhs then we need logic of loading all system tags as overwrite mode will generate lot of writes which can be problem
#SPECIAL NOTE on 05May2016 Naresh : this property suppose to do lot of optimization for small cases but we noticed one bug...as for smaller case it was going
# to cassandra to check node exists or not which is problematic....Period1 tag came pune and Period2 tag came PUNE so both were getting registered separately as 2 nodes 
# as cassandra where clause is caseinsensitive and we do not have solution there........so AVOID this property and always keep it zero for usecase of casesensitive tags upload
mtclient.tagsimilarto.triggerloadsystemtags.ifrowsgreaterthan=100000

#Used in data source file upload, when checking if tagcombinations already exist *for optimization
#if the tag combinations list size exceeds the given below value it will all the system combinations or else it will execute independent query for each combination 
mtclient.datasource.triggerloadsystemtagcombinations.ifrowsgreaterthan=1000

#-------------------------------------------------------------------------------
# MetricDefinition
#--------------------------------------------------------------------------------
# On click of Context Tag file upload against ContextMap there is logic which loads all system tags to check whether tag's uploaded in file is new or not
# Loading all system tags can be problematic so this flag will help us..If no of tags to be uploaded are more than flag value then only it will load all system tags
# If our input tags are in thousand then we do not need logic of loading all system tags as overwrite of thousand tags will work faster
# If our inpurt tags are in lakhs then we need logic of loading all system tags as overwrite mode will generate lot of writes which can be problem
mtclient.metricdefinition.contextmap.uploadfile.triggerloadsystemtags.ifrowsgreaterthan=100000
# PROCESS TO REGISTER CARTESAIN COMBINATIONS
# After cartesian we received N combinaton(Pune-Product1, Pune-Product2) which can be hundereds or millions...Each of these should be registered as 
# Node and isA NodeCombination( Without this we cannot use them in Graph)...Each node will be given RULE_BASED_ID(pipeseparated) which can be deciphered
# Each combination can be stored in isolation thats why introducing parallel logic
# If this property true then MAKE SURE JPPF driver and JPPF nodes bat files are running and they are configured properly
mtclient.parallel.metricdefinition.registercartesiancombination.runAcrossMachinesUsingJPPF=false
# Do not create more than 100 on 8GB machine because each batch will generate N queries and those are kept in memory before executing them in one SHOT..so chance of OUTOFMEMORY 
mtclient.parallel.metricdefinition.registercartesiancombination.jobsBatchSize=10000
mtclient.parallel.metricdefinition.registercartesiancombination.noofthreads=100
# As most of jobs are in batches so what happens system has completed many batches but failed in lats batch...so here we can configure i from where to start again instead of doing all again
# DO NOT CHANGE BATCH SIZE if you start from a particular index but can change no of threads
mtclient.parallel.metricdefinition.registercartesiancombination.lasttryfailednowstartfrom=0

#In metricdefintion excel we get Pool-Tag, Pool-Tag rows then it will get (unique tagpools) from that N rows and will apply batching on tagpools
#For example we have 50000 rows in input excel, then unique pools came out 1000,batch size=100 then it will operate on only 100 pools then next 100 pools then next 100 etc
mtclient.metricdefinition.step1.tagpoolsprocessing.fromexcel.batchSize=10000

#In metricContextSequence we get metricId and List of tagorTagCombinationIds, we break the tagorTagCombinationIdsList in Batches(chunks)
mtclient.metricdefinition.metrictagortagcombinationlistprocessing.batchSize=1000

# PROCESS TO SETUP CROSS MAP
# Previous process has registered all the combinations so we can ASSUME in this process nodecombinations has been registered with the RULE_BASED_ID
# Each combination when in crossmap hasNextLevel relations and terminals..so all that is done in this process
# 4 tables which holds a crossmap updated in this process crossmapCalculatesAndDown, crossmapCombinationUp, crossmapCombinationTerminal, crossmapNodeLinks
# If this property true then MAKE SURE JPPF driver and JPPF nodes bat files are running and they are configured properly
mtclient.parallel.metricdefinition.setupcrossmap.runAcrossMachinesUsingJPPF=false
# Do not create more than 100 on 8GB machine because each batch will generate N queries and those are kept in memory before executing them in one SHOT..so chance of OUTOFMEMORY 
mtclient.parallel.metricdefinition.setupcrossmap.jobsBatchSize=10000
mtclient.parallel.metricdefinition.setupcrossmap.noofthreads=1
# As most of jobs are in batches so what happens system has completed many batches but failed in lats batch...so here we can configure i from where to start again instead of doing all again
# DO NOT CHANGE BATCH SIZE if you start from a particular index but can change no of threads
mtclient.parallel.metricdefinition.setupcrossmap.lasttryfailednowstartfrom=0


# In Add metric we use to register sequence for each tag and tagcombinations in then sense if Pune-Group1 comes what all it will impact..So registering each sequence can go parallel
# If this property true then MAKE SURE JPPF driver and JPPF nodes bat files are running and they are configured properly
mtclient.parallel.metricdefinition.storetagsequence.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metricdefinition.storetagsequence.jobsBatchSize=10000
mtclient.parallel.metricdefinition.storetagsequence.noofthreads=10
# As most of jobs are in batches so what happens system has completed many batches but failed in lats batch...so here we can configure i from where to start again instead of doing all again
# DO NOT CHANGE BATCH SIZE if you start from a particular index but can change no of threads
mtclient.parallel.metricdefinition.storetagsequence.lasttryfailednowstartfrom=0

mtclient.parallel.metricdefinition.storetagcombinationsequence.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metricdefinition.storetagcombinationsequence.jobsBatchSize=10000
mtclient.parallel.metricdefinition.storetagcombinationsequence.noofthreads=1
# As most of jobs are in batches so what happens system has completed many batches but failed in lats batch...so here we can configure i from where to start again instead of doing all again
# DO NOT CHANGE BATCH SIZE if you start from a particular index but can change no of threads
mtclient.parallel.metricdefinition.storetagcombinationsequence.lasttryfailednowstartfrom=0

mtclient.parallel.metricdefinition.storetagcombinationanalytic.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metricdefinition.storetagcombinationanalytic.jobsBatchSize=10000
mtclient.parallel.metricdefinition.storetagcombinationanalytic.noofthreads=100
# As most of jobs are in batches so what happens system has completed many batches but failed in lats batch...so here we can configure i from where to start again instead of doing all again...
# DO NOT CHANGE BATCH SIZE if you start from a particular index but can change no of threads
mtclient.parallel.metricdefinition.storetagcombinationanalytic.lasttryfailednowstartfrom=0

#-----------------------------------------------------------------------------
# Cartesian Algorithm
# Suppose we need to cartesian of 5 lists : List<5> <List<4> List<2> List<6> List<7>  
#1 Cartesian PAIRING Sequential ALGORITHM
#     In this algorithm system will pick first list then will do cartesian with next list then output of that do cartesian with next list..this way till END 
#2 Cartesian PAIRING PARALLEL ALGORITHM
#     In this alogrihtm system will pairs of 2 then next pair and then next pair....parallel threads will do cartesain of each pair....after these threads
#     finish output of these will become pairs recursively.....this way keep doing till we have final cartesian  
#3 Cartesian INDEX BASED SEQUENTIAL ALGORITHM
#     In this algorithm we figure ou How many combinations we need to prepare then each combination we prepare by getting elements at index from lists
#     Index for each list is a mathematical calculation....so simple getting elements from list and building one combination      
#4 Cartesian INDEX BASED PARALLEL ALGORITHM
#     In this instead of building combination one by one we do parallely...Each combination can be built parallel depending on threads.....Each thread will build 
#     one combination and exit...so we keep pushing jobs in queue and threads keep picking those....So for big case if we push all jobs in queue then it 
#     gives out of memory...SO no of jobs and no of threads is configurable parameter     
#5 Cartesian INDEX BASED PARALLEL ACROSS MACHINE ALGORITHM
#      Same as 4 only difference is will run across machines For this JPPF driver and JPPF multiple nodes MUST BE RUNNING OTHERWISE will fail
#      MAKE SURE JPPF driver and JPPF nodes bat files are running and they are configured properly
#-----------------------------------------------------------------------------
mtclient.cartesian.whichalogrithm=1
mtclient.cartesian.indexbasedparallelalgo.jobsBatchSize=100000
# IF set to -1 then system will automatically decide on threads but if more than -1 given then those many threads will be created
mtclient.cartesian.indexbasedparallelalgo.noofthreads=-1


#-------------------------------------------------------------------------------
# DataFetch
#--------------------------------------------------------------------------------
# In datafetch for each combination it search for value in RawDataSet model SO if there are around 20000 to 60000 combinations then searching that in 
# in rawdataset requires PARALLEL PROCESSING...So system will pick combinations in batch size and then will give to configured threads
#
mtclient.parallel.datafetch.combinationsearchindataset.jobsBatchSize=100000
mtclient.parallel.datafetch.combinationsearchindataset.noofthreads=1

#-------------------------------------------------------------------------------
# TagCombinationService
#--------------------------------------------------------------------------------
# In TagCombinationService while creating SOLR documents logic is to do in batches to handle big cases..Do not attempt very big value it will give 
# error also do not take very low, time will increase..If given -1 then it store all in one shot..
# Observation : tried Creating Solr Documents with 50k batch was taking 8 minutes but then changed to 5k then was taking more than 50 minutes
mtclient.tagcombinationservice.createsolrdocuments.batchSize=50000

#-------------------------------------------------------------------------------
# MetricResultOrTerminalDataService
#--------------------------------------------------------------------------------
# In MetricResultOrTerminalDataService while creating logic is to do in batches to handle big cases..Do not attempt very big value it will give 
# error..If given -1 then it store all in one shot
mtclient.metricresultorterminaldataservice.create.batchSize=5000

#-------------------------------------------------------------------------------
# Calculation
#--------------------------------------------------------------------------------
# Parallel calculation of metrics
mtclient.parallel.metriccalculation.parallelmetrics.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metriccalculation.parallelmetrics.jobsBatchSize=100
mtclient.parallel.metriccalculation.parallelmetrics.noofthreads=100

# Parallel calculation of metric periods
mtclient.parallel.metriccalculation.parallelperiods.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metriccalculation.parallelperiods.jobsBatchSize=100
mtclient.parallel.metriccalculation.parallelperiods.noofthreads=1

# Parallel calculation of metric period Contexts
mtclient.parallel.metriccalculation.parallelcontexts.runAcrossMachinesUsingJPPF=false
mtclient.parallel.metriccalculation.parallelcontexts.jobsBatchSize=100
mtclient.parallel.metriccalculation.parallelcontexts.noofthreads=1

mtclient.dataconnector.jar.path=D:\\dist\\

#-------------------------------------------------------------------------------
# QueueMessage
#--------------------------------------------------------------------------------
# This proeprty will tell system that in messageinfo a period, metric if context is large i.e > configuredvalue then store that OUTSIDE XML in queuemessagemetriccontext table 
mtclient.queuemessage.messageinfo.maxcontextSizeEligibleForStoringOutsideXMLinATable=10000

# This property tell batch size for getting section metric result list
mtclient.outputSectionMetricResultService.get.batchSize=1000

# Custom Export Uploaded Excel file folder path
mtclient.uploadedfiles.folder.location=D:\\uploadedFiles\\Reports\\
#mtclient.uploadedfiles.folder.location=\\\\DESKTOP-SPG5GRD\\d\\sandbox\\uploadedFiles\\Reports\\

# Email Id to Intimate that user uploaded new Files
mtclient.sysadmin.emailId=bhushan.phadnis@mezzure.com

# Email URL Prefix
mtclient.email.content.insideurlprefix=http://localhost:8080

# SendSMS configuration
# SMSUtil class will use this variable to decide whether sendSMS using Local attached modem or using Public Gateway Service Provider.
mtclient.sendsms.uselocalmodem=false
# SMS gateway settings
mtclient.sendsms.smsGatewayURL=http://dev.mezzure.com:8080/useraccount/sendSMS.do
#mtclient.sendsms.smsGatewayURL=http://api.mVaayoo.com/mvaayooapi/MessageCompose 
#https://bulksms.vsms.net/eapi/submission/send_sms/2/2.0
mtclient.sendsms.smsGatewayUsername=anandkchaugule@gmail.com
mtclient.sendsms.smsGatewayPassword=password123

# Send sms has been implemeted using dongle with sim for that usecase we need these properties, will not used if we use a Gateway Service Provider
#SMS Center number
#TATA DOCOMO : +919032055002
mtclient.sendsms.gsmGatewaySMScNumber=+919032055002
mtclient.sendsms.gsmGatewayComPortNumber=COM12

# Google Notification Server for sending notification to mobile devices
# GCM Server notifications configuration
mtclient.sendnotifications.gcm.serverurl=https://android.googleapis.com/gcm/send
mtclient.sendnotifications.gcm.authkey=AIzaSyDvRZq3VmTETGrZrZHWK4206VFHtPzLDjQ

# SOLR Directory Location
mtclient.solr.directory.location=D:\\Softwares\\solr-6.3.0\\server\\solr

# SOLR Data Directory Location
mtclient.solr.data.directory.location=D:\\var\\lucene\\solr\\

# Download Sample CSV file from below folder
mtclient.download.tagpool.file.folder.location=D:/downloadFiles/sampleTagpoolFile.csv
mtclient.download.combination.file.folder.location=D:/downloadFiles/sampleCombinationFile.csv

# Wth this flag system will enable disable functionalties on various pages. IF SMB/ODANAX deployment then set false OTHERWISE for p&g set it to true
# FOR example in SMB normal user we wanted all Screcard Section level edit/delete disabled BUT for admin we wanted so this flag help that
#             But for P&G we wanted all user to enjoy controls on section on scorecard  
mtclient.isnotsmbdeployment=true